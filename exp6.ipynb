{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "\n",
    "data.head(50)\n",
    "\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "data.head(50)\n",
    "\n",
    "print(\"Unique words in corpus:\", data['Word'].nunique())\n",
    "print(\"Unique tags in corpus:\", data['Tag'].nunique())\n",
    "\n",
    "words=list(data['Word'].unique())\n",
    "words.append(\"ENDPAD\")\n",
    "tags=list(data['Tag'].unique())\n",
    "\n",
    "print(\"Unique tags are:\", tags)\n",
    "\n",
    "num_words = len(words)\n",
    "num_tags = len(tags)\n",
    "\n",
    "num_words\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences\n",
    "\n",
    "len(sentences)\n",
    "\n",
    "sentences[0]\n",
    "\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "word2idx\n",
    "\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.show()\n",
    "\n",
    "X1 = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "\n",
    "type(X1[0])\n",
    "\n",
    "X1[0]\n",
    "\n",
    "max_len = 50\n",
    "\n",
    "# **pad_sequences example**\n",
    "\n",
    "nums = [[1], [2, 3], [4, 5, 6]]\n",
    "sequence.pad_sequences(nums)\n",
    "\n",
    "nums = [[1], [2, 3], [4, 5, 6]]\n",
    "sequence.pad_sequences(nums,maxlen=2)\n",
    "\n",
    "X = sequence.pad_sequences(maxlen=max_len,\n",
    "                  sequences=X1, padding=\"post\",\n",
    "                  value=num_words-1)\n",
    "\n",
    "X[0]\n",
    "\n",
    "y1 = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "\n",
    "y = sequence.pad_sequences(maxlen=max_len,\n",
    "                  sequences=y1,\n",
    "                  padding=\"post\",\n",
    "                  value=tag2idx[\"O\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "X_train[0]\n",
    "\n",
    "y_train[0]\n",
    "\n",
    "input_word = layers.Input(shape=(max_len,))\n",
    "embedding_layer=layers.Embedding(input_dim=num_words,output_dim=50,input_length=max_len)(input_word)\n",
    "dropout_layer=layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "bidirectional_lstm=layers.Bidirectional(\n",
    "    layers.LSTM(units=100,return_sequences=True,\n",
    "                recurrent_dropout=0.1))(dropout_layer)\n",
    "output=layers.TimeDistributed(\n",
    "      layers.Dense(num_tags,activation=\"softmax\"))(bidirectional_lstm)\n",
    "model = Model(input_word, output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test,y_test),\n",
    "    batch_size=32, \n",
    "    epochs=3,\n",
    ")\n",
    "\n",
    "metrics = pd.DataFrame(model.history.history)\n",
    "metrics.head()\n",
    "\n",
    "metrics[['accuracy','val_accuracy']].plot()\n",
    "\n",
    "metrics[['loss','val_loss']].plot()\n",
    "\n",
    "i = 20\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "y_true = y_test[i]\n",
    "print(\"{:15}{:5}\\t {}\\n\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(\"-\" *30)\n",
    "for w, true, pred in zip(X_test[i], y_true, p[0]):\n",
    "    print(\"{:15}{}\\t{}\".format(words[w-1], tags[true], tags[pred]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
